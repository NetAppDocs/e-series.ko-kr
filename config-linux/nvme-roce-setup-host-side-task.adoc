---
permalink: config-linux/nvme-roce-setup-host-side-task.html 
sidebar: sidebar 
keywords: express linux configuration, software configuration, linux host, E2800, E5700, EF300, EF600, E-Series, eseries 
summary: RoCE 환경의 NVMe 이니시에이터 구성에는 RDMA 코어 및 NVMe-CLI 패키지 설치 및 구성, 이니시에이터 IP 주소 구성, 호스트에 NVMe-oF 계층 설정 등이 포함됩니다. 
---
= 호스트 측에서 NVMe over RoCE를 설정합니다
:allow-uri-read: 
:icons: font
:imagesdir: ../media/


[role="lead"]
RoCE 환경의 NVMe 이니시에이터 구성에는 RDMA 코어 및 NVMe-CLI 패키지 설치 및 구성, 이니시에이터 IP 주소 구성, 호스트에 NVMe-oF 계층 설정 등이 포함됩니다.

.필요한 것
호환되는 최신 RHEL 7, RHEL 8, RHEL 9, SUSE Linux Enterprise Server 12 또는 15 서비스 팩 운영 체제를 실행해야 합니다. 를 참조하십시오 https://mysupport.netapp.com/matrix["NetApp 상호 운용성 매트릭스 툴"^] 최신 요구 사항의 전체 목록을 확인하십시오.

.단계
. RDMA 및 NVMe-CLI 패키지를 설치합니다.
+
* SLES 12 또는 SLES 15 *

+
[listing]
----

# zypper install rdma-core
# zypper install nvme-cli
----
+
RHEL 7, RHEL 8 및 RHEL 9 *

+
[listing]
----

# yum install rdma-core
# yum install nvme-cli
----
. RHEL 8 및 RHEL 9의 경우 네트워크 스크립트를 설치합니다.
+
RHEL 8 *

+
[listing]
----
# yum install network-scripts
----
+
RHEL 9 *

+
[listing]
----
# yum install NetworkManager-initscripts-updown
----
. 호스트를 스토리지에 구성하는 데 사용되는 호스트 NQN을 가져옵니다.
+
[listing]
----
# cat /etc/nvme/hostnqn
----
. NVMe over RoCE를 연결하는 데 사용되는 이더넷 포트에서 IPv4 IP 주소를 설정합니다. 각 네트워크 인터페이스에 대해 해당 인터페이스에 대한 다양한 변수가 포함된 구성 스크립트를 생성합니다.
+
이 단계에서 사용되는 변수는 서버 하드웨어와 네트워크 환경을 기반으로 합니다. 이 변수에는 IPADDR, Gateway 등이 있다. 다음은 SLES 및 RHEL의 예제 지침입니다.

+
* SLES 12 및 SLES 15 *

+
다음 내용으로 예제 파일 '/etc/sysconfig/network/ifcfg-eth4'를 생성합니다.

+
[listing]
----
BOOTPROTO='static'
BROADCAST=
ETHTOOL_OPTIONS=
IPADDR='192.168.1.87/24'
GATEWAY='192.168.1.1'
MTU=
NAME='MT27800 Family [ConnectX-5]'
NETWORK=
REMOTE_IPADDR=
STARTMODE='auto'
----
+
그런 다음 예제 파일 '/etc/sysconfig/network/ifcfg-eth5'를 생성합니다.

+
[listing]
----
BOOTPROTO='static'
BROADCAST=
ETHTOOL_OPTIONS=
IPADDR='192.168.2.87/24'
GATEWAY='192.168.2.1'
MTU=
NAME='MT27800 Family [ConnectX-5]'
NETWORK=
REMOTE_IPADDR=
STARTMODE='auto'
----
+
RHEL 7 또는 RHEL 8 *

+
다음 내용으로 예제 파일 '/etc/sysconfig/network-scripts/ifcfg-eth4'를 생성합니다.

+
[listing]
----
BOOTPROTO='static'
BROADCAST=
ETHTOOL_OPTIONS=
IPADDR='192.168.1.87/24’
GATEWAY='192.168.1.1'
MTU=
NAME='MT27800 Family [ConnectX-5]'
NETWORK=
REMOTE_IPADDR=
STARTMODE='auto'
----
+
그런 다음 예제 파일 '/etc/sysconfig/network-scripts/ifcfg-eth5'를 생성합니다.

+
[listing]
----
BOOTPROTO='static'
BROADCAST=
ETHTOOL_OPTIONS=
IPADDR='192.168.2.87/24'
GATEWAY='192.168.2.1'
MTU=
NAME='MT27800 Family [ConnectX-5]'
NETWORK=
REMOTE_IPADDR=
STARTMODE='auto'
----
+
RHEL 9 *

+
를 사용합니다 `nmtui` 도구를 사용하여 연결을 활성화하고 편집합니다. 다음은 예제 파일입니다 `/etc/NetworkManager/system-connections/eth4.nmconnection` 이 도구는 다음을 생성합니다.

+
[listing]
----

[connection]
id=eth4
uuid=<unique uuid>
type=ethernet
interface-name=eth4

[ethernet]
mtu=4200

[ipv4]
address1=192.168.1.87/24
method=manual

[ipv6]
addr-gen-mode=default
method=auto

[proxy]
----
+
다음은 예제 파일입니다 `/etc/NetworkManager/system-connections/eth5.nmconnection` 이 도구는 다음을 생성합니다.

+
[listing]
----

[connection]
id=eth5
uuid=<unique uuid>
type=ethernet
interface-name=eth5

[ethernet]
mtu=4200

[ipv4]
address1=192.168.2.87/24
method=manual

[ipv6]
addr-gen-mode=default
method=auto

[proxy]
----
. 네트워크 인터페이스를 활성화합니다.
+
[listing]
----

# ifup eth4
# ifup eth5
----
. 호스트에서 NVMe-oF 계층을 설정합니다. 다음 파일을 '/etc/modules-load.d/'에서 생성하여 NVMe-RDMA 커널 모듈을 로드하고 재부팅 후에도 커널 모듈이 항상 켜져 있는지 확인합니다.
+
[listing]
----

# cat /etc/modules-load.d/nvme-rdma.conf
  nvme-rdma
----
+
"NVMe-RDMA" 커널 모듈이 로드되었는지 확인하려면 다음 명령을 실행합니다.

+
[listing]
----
# lsmod | grep nvme
nvme_rdma              36864  0
nvme_fabrics           24576  1 nvme_rdma
nvme_core             114688  5 nvme_rdma,nvme_fabrics
rdma_cm               114688  7 rpcrdma,ib_srpt,ib_srp,nvme_rdma,ib_iser,ib_isert,rdma_ucm
ib_core               393216  15 rdma_cm,ib_ipoib,rpcrdma,ib_srpt,ib_srp,nvme_rdma,iw_cm,ib_iser,ib_umad,ib_isert,rdma_ucm,ib_uverbs,mlx5_ib,qedr,ib_cm
t10_pi                 16384  2 sd_mod,nvme_core
----

